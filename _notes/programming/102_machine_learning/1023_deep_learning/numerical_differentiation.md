---
title: "数値微分法"
date: "2019-11-06T01:01:30+09:00"
excerpt: "数値微分法について"
tag: ["Python"]
updatedAt: "2019-11-06T01:01:30+09:00"
author:
  name: Tatsuroh Wakasugi
  picture: "/assets/blog/authors/WAT.jpg"
---

前述のフィードフォワードニューラルネットワークにおいて、最適な重み行列 w,v の値を求めるために指標となる誤差関数を、教師あり学習の章でも利用した平均誤差エントロピー誤差を利用し、以下の式で定義する。

$$
\tag{1}  E( { \bf w } ,{ \bf v } ) = - \frac{1}{N} \sum_{n=0}^{N-1} \sum_{k=0}^{K-1} t_{nk} \log (y_{nk})
$$

t は目標値、y はニューラルネットワークモデルの出力値である。この 2 つの誤差が大きいほど、誤差関数の値も大きくなる。

式(1)は ${ \bf w } ,{ \bf v }$ を入力とするが、この時どのような w,v を入力すれば誤差関数が最も小さくなるか？を考えた時、教師あり学習のところでも出てきた勾配法を用いて考えてみる。

勾配法の時は偏微分を計算して算出した式を利用していたが、ここでは微分の定義を利用した、近似を利用して偏微分を行なってみよう。

まず誤差関数を ${ \bf w }$ で偏微分することを考えてみる。

関数 f(x)を x で微分した値は、ある微小な値 ε を用いると以下のように表される。

$$
\tag{2}  \frac{　df(x) }{dx} = \frac{f(x + \epsilon )-f(x)}{ \epsilon }
$$

これにより、誤差関数 $ E( { \bf w } ,{ \bf v } )$ で偏微分した値は以下のようになる。

$$
\tag{3}  \frac{　\partial E( { \bf w } ,{ \bf v } ) }{ \partial { \bf w } } = \frac{ E( { \bf w } + \epsilon  ,{ \bf v } )-E( { \bf w }  ,{ \bf v } )}{ \epsilon }
$$

式(3)だが、${ \bf w }$ には実際にはパラメータが複数ある。例として、${ \bf w }$ に $w_{0},w_{1},w_{2}$ のパラメータがあるとすると、まず $w_{0}$ での偏微分は以下のようになる。

$$
\tag{4}  \frac{　\partial E( { \bf w } ,{ \bf v } ) }{ \partial { \bf w } } \Biggr| _{w_{0},w_{1},w_{2} }= \frac{ E( ( w_{0} + \epsilon, w_{1}, w_{2})  ,{ \bf v } )-E( ( w_{0}, w_{1}, w_{2})  ,{ \bf v } )}{ \epsilon }
$$

$w_{1},w_{2}$ に対しても同じことを行い、また${ \bf v }$のパラメータ全てに対しても行う。

つまりは、${ \bf w }$ と ${ \bf v }$ のパラメータ全てに対して勾配法を行い、誤差関数を小さくする最適なパタメータの組み合わせを求めていく、というものである。

この手法の難点は、パラメータの数が多いと計算時間が膨大になってしまうという点である。ニューラルネットワークでは重み行列のパラメータの数が多いので、時間も長くなりがちになる。

コードでの実装例を[こちら](https://github.com/WAT36/python/blob/master/machine_learning/deeplearning/numerical_differentialation.ipynb)の Notebook に記載するが、回数にもよるが時間が長くかかりやすいこともわかる。

ここで、別の方法として述べられているものを次に記載する。
